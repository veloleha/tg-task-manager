import asyncio
import logging
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import json
from core.redis_client import redis_client
from config.settings import settings

logger = logging.getLogger(__name__)

class MessageAggregator:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∞–≤—Ç–æ–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    
    def __init__(self):
        self.redis = redis_client
        self.aggregation_timeout = 60  # 1 –º–∏–Ω—É—Ç–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞–¥–∞—á–∏
        self.pending_tasks: Dict[int, asyncio.Task] = {}
        self.created_tasks: Dict[int, str] = {}  # user_id -> task_id –¥–ª—è —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
    
    async def add_message(self, user_id: int, message_data: dict) -> bool:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –æ—á–µ—Ä–µ–¥—å –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        –°–æ–∑–¥–∞–µ—Ç –∑–∞–¥–∞—á—É –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏, –æ–±–Ω–æ–≤–ª—è–µ—Ç –ø—Ä–∏ –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö
        """
        try:
            logger.info(f"[AGGREGATOR][START] add_message called for user {user_id}")
            logger.info(f"[AGGREGATOR][START] Message data keys: {list(message_data.keys())}")
            logger.info(f"[AGGREGATOR][START] Current created_tasks: {list(self.created_tasks.keys())}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            if user_id in self.created_tasks:
                logger.info(f"[AGGREGATOR][UPDATE] Found existing task for user {user_id}")
                # –ó–∞–¥–∞—á–∞ —É–∂–µ —Å–æ–∑–¥–∞–Ω–∞, –¥–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π
                logger.info(f"[AGGREGATOR][UPDATE] Getting pending messages...")
                existing_messages = await self._get_pending_messages(user_id)
                logger.info(f"[AGGREGATOR][UPDATE] Got {len(existing_messages)} existing messages")
                
                existing_messages.append(message_data)
                logger.info(f"[AGGREGATOR][UPDATE] Appended new message, total: {len(existing_messages)}")
                
                logger.info(f"[AGGREGATOR][UPDATE] Saving pending messages...")
                await self._save_pending_messages(user_id, existing_messages)
                logger.info(f"[AGGREGATOR][UPDATE] Pending messages saved")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–¥–∞—á—É
                logger.info(f"[AGGREGATOR][UPDATE] Updating existing task...")
                await self._update_existing_task(user_id, existing_messages)
                logger.info(f"[AGGREGATOR][UPDATE] Task updated")
                
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–∞–π–º–µ—Ä
                logger.info(f"[AGGREGATOR][UPDATE] Resetting timer...")
                await self._reset_aggregation_timer(user_id)
                logger.info(f"[AGGREGATOR][UPDATE] Timer reset")
                
                logger.info(f"[AGGREGATOR][UPDATE] ‚úÖ Updated existing task for user {user_id}. Total messages: {len(existing_messages)}")
                return True
            else:
                logger.info(f"[AGGREGATOR][CREATE] No existing task for user {user_id}, creating new one")
                # –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ - —Å–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ
                logger.info(f"[AGGREGATOR][CREATE] Saving pending messages...")
                await self._save_pending_messages(user_id, [message_data])
                logger.info(f"[AGGREGATOR][CREATE] Pending messages saved")
                
                logger.info(f"[AGGREGATOR][CREATE] Creating aggregated task...")
                task_id = await self._create_aggregated_task(user_id)
                logger.info(f"[AGGREGATOR][CREATE] Task creation result: {task_id}")
                
                if task_id:
                    logger.info(f"[AGGREGATOR][CREATE] Adding task to created_tasks dict")
                    self.created_tasks[user_id] = task_id
                    logger.info(f"[AGGREGATOR][CREATE] Starting aggregation timer...")
                    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–∞–π–º–µ—Ä –¥–ª—è –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
                    await self._start_aggregation_timer(user_id)
                    logger.info(f"[AGGREGATOR][CREATE] Timer started")
                    
                    logger.info(f"[AGGREGATOR][CREATE] ‚úÖ Created task immediately for user {user_id}: {task_id}")
                    return True
                else:
                    logger.error(f"[AGGREGATOR][CREATE] ‚ùå Failed to create task for user {user_id}")
                    return False
                
        except Exception as e:
            logger.error(f"[AGGREGATOR][ERROR] ‚ùå Error adding message to aggregation: {e}", exc_info=True)
            return False
    
    async def _get_pending_messages(self, user_id: int) -> List[dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–∂–∏–¥–∞—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            await self.redis._ensure_connection()
            messages_data = await self.redis.conn.get(f"pending_messages:{user_id}")
            if not messages_data:
                return []
            
            if isinstance(messages_data, bytes):
                messages_data = messages_data.decode('utf-8')
            
            return json.loads(messages_data)
            
        except Exception as e:
            logger.error(f"Error getting pending messages: {e}")
            return []
    
    async def _save_pending_messages(self, user_id: int, messages: List[dict]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–∂–∏–¥–∞—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        try:
            await self.redis._ensure_connection()
            await self.redis.conn.setex(
                f"pending_messages:{user_id}",
                self.aggregation_timeout + 60,  # TTL —á—É—Ç—å –±–æ–ª—å—à–µ —Ç–∞–π–º–∞—É—Ç–∞
                json.dumps(messages)
            )
        except Exception as e:
            logger.error(f"Error saving pending messages: {e}")
    
    async def _start_aggregation_timer(self, user_id: int):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–∞–π–º–µ—Ä –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        # –û—Ç–º–µ–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–∞–π–º–µ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
        if user_id in self.pending_tasks:
            self.pending_tasks[user_id].cancel()
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç–∞–π–º–µ—Ä
        task = asyncio.create_task(self._aggregation_timer(user_id))
        self.pending_tasks[user_id] = task
    
    async def _reset_aggregation_timer(self, user_id: int):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Ç–∞–π–º–µ—Ä –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ (–ø—Ä–æ–¥–ª–µ–≤–∞–µ—Ç –æ–∂–∏–¥–∞–Ω–∏–µ)"""
        await self._start_aggregation_timer(user_id)
    
    async def _aggregation_timer(self, user_id: int):
        """–¢–∞–π–º–µ—Ä –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π (—Ç–µ–ø–µ—Ä—å –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π)"""
        try:
            await asyncio.sleep(self.aggregation_timeout)
            
            # –í—Ä–µ–º—è –∏—Å—Ç–µ–∫–ª–æ - –æ—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
            await self._cleanup_aggregation_data(user_id)
            
        except asyncio.CancelledError:
            logger.debug(f"Aggregation timer cancelled for user {user_id}")
        except Exception as e:
            logger.error(f"Error in aggregation timer: {e}")
        finally:
            # –£–±–∏—Ä–∞–µ–º –∑–∞–¥–∞—á—É –∏–∑ —Å–ø–∏—Å–∫–∞ –∞–∫—Ç–∏–≤–Ω—ã—Ö
            if user_id in self.pending_tasks:
                del self.pending_tasks[user_id]
            if user_id in self.created_tasks:
                del self.created_tasks[user_id]
    
    async def _create_aggregated_task(self, user_id: int):
        """–°–æ–∑–¥–∞–µ—Ç –∑–∞–¥–∞—á—É –∏–∑ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
        try:
            messages = await self._get_pending_messages(user_id)
            if not messages:
                logger.warning(f"No messages found for aggregation for user {user_id}")
                return
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã —Å–æ–æ–±—â–µ–Ω–∏–π
            combined_text = self._combine_messages(messages)
            
            # –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∫–∞–∫ –æ—Å–Ω–æ–≤—É
            base_message = messages[0]
            
            # –°–æ–∑–¥–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∑–∞–¥–∞—á—É
            aggregated_task = {
                "message_id": base_message["message_id"],
                "chat_id": base_message["chat_id"],
                "chat_title": base_message["chat_title"],
                "chat_type": base_message["chat_type"],
                "user_id": base_message["user_id"],
                "first_name": base_message["first_name"],
                "last_name": base_message["last_name"],
                "username": base_message["username"],
                "language_code": base_message["language_code"],
                "is_bot": base_message["is_bot"],
                "text": combined_text,
                "status": "unreacted",
                "task_number": None,
                "assignee": None,
                "task_link": None,
                "reply": None,
                "created_at": base_message["created_at"],
                "updated_at": None,
                "aggregated": True,
                "message_count": len(messages),
                "aggregation_period": self.aggregation_timeout
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–¥–∞—á—É
            logger.info(f"üîÑ –≠–¢–ê–ü 1: –ù–∞—á–∏–Ω–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –≤ Redis...")
            task_id = await self._save_task_to_redis(aggregated_task)
            logger.info(f"‚úÖ –≠–¢–ê–ü 1 –ó–ê–í–ï–†–®–ï–ù: –ó–∞–¥–∞—á–∞ {task_id} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ Redis")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            await asyncio.sleep(0.1)
            
            # –ü—É–±–ª–∏–∫—É–µ–º —Å–æ–±—ã—Ç–∏–µ –æ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–µ
            logger.info(f"üîÑ –≠–¢–ê–ü 2: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–≥–Ω–∞–ª –≤ PubSub –∫–∞–Ω–∞–ª 'new_tasks'...")
            await self._publish_task_event(task_id)
            logger.info(f"[USERBOT][STEP 2] –û—Ç–ø—Ä–∞–≤–ª–µ–Ω —Å–∏–≥–Ω–∞–ª –ø–æ Pub/Sub –æ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–µ: {task_id}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏
            await self._increment_counter("unreacted")
            
            # –û—á–∏—â–∞–µ–º –æ–∂–∏–¥–∞—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            await self.redis._ensure_connection()
            await self.redis.conn.delete(f"pending_messages:{user_id}")
            
            logger.info(f"Created aggregated task {task_id} from {len(messages)} messages for user {user_id}")
            
            return task_id
            
        except Exception as e:
            logger.error(f"Error creating aggregated task: {e}")
            return None
    
    async def _update_existing_task(self, user_id: int, messages: list):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–¥–∞—á—É –Ω–æ–≤—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏"""
        try:
            task_id = self.created_tasks.get(user_id)
            if not task_id:
                logger.error(f"No task ID found for user {user_id}")
                return False
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã —Å–æ–æ–±—â–µ–Ω–∏–π
            combined_text = self._combine_messages(messages)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –≤ Redis
            await self.redis._ensure_connection()
            task_key = f"task:{task_id}"
            task_data = await self.redis.get_task(task_id)
            
            if task_data:
                task_data['text'] = combined_text
                task_data['message_count'] = len(messages)
                task_data['updated_at'] = datetime.utcnow().isoformat()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∑–∞–¥–∞—á—É
                await self.redis.conn.set(task_key, json.dumps(task_data))
                
                # –ü—É–±–ª–∏–∫—É–µ–º —Å–æ–±—ã—Ç–∏–µ –æ–± –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏
                await self._publish_task_update_event(task_id, combined_text)
                
                logger.info(f"Updated task {task_id} with {len(messages)} messages")
                return True
            else:
                logger.error(f"Task {task_id} not found in Redis")
                return False
                
        except Exception as e:
            logger.error(f"Error updating existing task: {e}")
            return False
    
    async def _publish_task_event(self, task_id: str):
        """–ü—É–±–ª–∏–∫—É–µ—Ç —Å–æ–±—ã—Ç–∏–µ –æ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–µ"""
        try:
            logger.info(f"[PUBSUB][PUBLISH] Starting task event publication for task: {task_id}")
            await self.redis._ensure_connection()
            logger.info(f"[PUBSUB][PUBLISH] Redis connection ensured")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–æ–±—ã—Ç–∏—è
            task_data = await self.redis.get_task(task_id)
            if not task_data:
                logger.error(f"[PUBSUB][PUBLISH] ‚ùå Cannot publish event for non-existent task: {task_id}")
                return
            logger.info(f"[PUBSUB][PUBLISH] Task data retrieved for event: {len(task_data)} fields")
            
            event_data = {
                'task_id': task_id,
                'type': 'new_task',
                'user_id': int(task_data.get('user_id', 0)),
                'username': task_data.get('username', ''),
                'text': task_data.get('text', '')
            }
            logger.info(f"[PUBSUB][PUBLISH] Event data prepared: {event_data}")
            
            event_json = json.dumps(event_data)
            logger.info(f"[PUBSUB][PUBLISH] Event serialized to JSON: {len(event_json)} chars")
            
            result = await self.redis.conn.publish('new_tasks', event_json)
            logger.info(f"[PUBSUB][PUBLISH] ‚úÖ Published to 'new_tasks' channel, subscribers notified: {result}")
            logger.info(f"[USERBOT][STEP 2] Published event to new_tasks: {event_data}")
            
        except Exception as e:
            logger.error(f"[PUBSUB][PUBLISH] ‚ùå Error publishing task event: {e}", exc_info=True)
    
    async def _publish_task_update_event(self, task_id: str, updated_text: str):
        """–ü—É–±–ª–∏–∫—É–µ—Ç —Å–æ–±—ã—Ç–∏–µ –æ–± –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏"""
        try:
            logger.info(f"[PUBSUB][PUBLISH] Starting task update event publication for task: {task_id}")
            await self.redis._ensure_connection()
            logger.info(f"[PUBSUB][PUBLISH] Redis connection ensured")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–æ–±—ã—Ç–∏—è
            task_data = await self.redis.get_task(task_id)
            if not task_data:
                logger.error(f"[PUBSUB][PUBLISH] ‚ùå Cannot publish event for non-existent task: {task_id}")
                return
            logger.info(f"[PUBSUB][PUBLISH] Task data retrieved for event: {len(task_data)} fields")
            
            event_data = {
                'task_id': task_id,
                'type': 'task_update',
                'user_id': int(task_data.get('user_id', 0)),
                'username': task_data.get('username', ''),
                'updated_text': updated_text
            }
            logger.info(f"[PUBSUB][PUBLISH] Event data prepared: {event_data}")
            
            event_json = json.dumps(event_data)
            logger.info(f"[PUBSUB][PUBLISH] Event serialized to JSON: {len(event_json)} chars")
            
            result = await self.redis.conn.publish('task_updates', event_json)
            logger.info(f"[PUBSUB][PUBLISH] ‚úÖ Published to 'task_updates' channel, subscribers notified: {result}")
            logger.info(f"[USERBOT][STEP 2.1] Published task update event for task {task_id}")
            
        except Exception as e:
            logger.error(f"[PUBSUB][PUBLISH] ‚ùå Error publishing task update event: {e}", exc_info=True)
    
    async def _cleanup_aggregation_data(self, user_id: int):
        """–û—á–∏—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–µ—Ä–∏–æ–¥–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π"""
        try:
            await self.redis._ensure_connection()
            # –û—á–∏—â–∞–µ–º –æ–∂–∏–¥–∞—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            await self.redis.conn.delete(f"pending_messages:{user_id}")
            logger.info(f"Cleaned up aggregation data for user {user_id}")
        except Exception as e:
            logger.error(f"Error cleaning up aggregation data: {e}")
    
    def _combine_messages(self, messages: List[dict]) -> str:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç—ã —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –æ–¥–∏–Ω"""
        try:
            texts = []
            for i, msg in enumerate(messages, 1):
                text = msg.get("text", "").strip()
                if text:
                    if len(messages) > 1:
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä —Å–æ–æ–±—â–µ–Ω–∏—è –µ—Å–ª–∏ –∏—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ
                        texts.append(f"[{i}] {text}")
                    else:
                        texts.append(text)
            
            combined = "\n\n".join(texts)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
            if len(messages) > 1:
                timestamp_info = f"\n\nüìù –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –∑–∞ {self.aggregation_timeout // 60} –º–∏–Ω—É—Ç"
                combined += timestamp_info
            
            return combined
            
        except Exception as e:
            logger.error(f"Error combining messages: {e}")
            return "–û—à–∏–±–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π"
    
    async def force_create_task(self, user_id: int) -> Optional[str]:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–µ—Ç –∑–∞–¥–∞—á—É –∏–∑ –æ–∂–∏–¥–∞—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
        try:
            # –û—Ç–º–µ–Ω—è–µ–º —Ç–∞–π–º–µ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
            if user_id in self.pending_tasks:
                self.pending_tasks[user_id].cancel()
                del self.pending_tasks[user_id]
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É
            return await self._create_aggregated_task(user_id)
            
        except Exception as e:
            logger.error(f"Error force creating task: {e}")
            return None
    
    async def cancel_aggregation(self, user_id: int):
        """–û—Ç–º–µ–Ω—è–µ—Ç –∞–≥—Ä–µ–≥–∞—Ü–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            # –û—Ç–º–µ–Ω—è–µ–º —Ç–∞–π–º–µ—Ä
            if user_id in self.pending_tasks:
                self.pending_tasks[user_id].cancel()
                del self.pending_tasks[user_id]
            
            # –û—á–∏—â–∞–µ–º –æ–∂–∏–¥–∞—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            await self.redis._ensure_connection()
            await self.redis.conn.delete(f"pending_messages:{user_id}")
            
            logger.info(f"Cancelled aggregation for user {user_id}")
            
        except Exception as e:
            logger.error(f"Error cancelling aggregation: {e}")
    
    async def get_aggregation_status(self, user_id: int) -> dict:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            messages = await self._get_pending_messages(user_id)
            has_timer = user_id in self.pending_tasks
            
            return {
                "user_id": user_id,
                "pending_messages": len(messages),
                "has_active_timer": has_timer,
                "timeout_seconds": self.aggregation_timeout
            }
            
        except Exception as e:
            logger.error(f"Error getting aggregation status: {e}")
            return {"error": str(e)}
    
    async def cleanup_expired_aggregations(self):
        """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–µ–∫—à–∏–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ (–ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞)"""
        try:
            await self.redis._ensure_connection()
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–ª—é—á–∏ –æ–∂–∏–¥–∞—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
            keys = []
            async for key in self.redis.conn.scan_iter("pending_messages:*"):
                keys.append(key)
            
            for key in keys:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º TTL
                ttl = await self.redis.conn.ttl(key)
                if ttl <= 0:  # –ö–ª—é—á –∏—Å—Ç–µ–∫ –∏–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                    await self.redis.conn.delete(key)
                    logger.debug(f"Cleaned up expired aggregation: {key}")
            
        except Exception as e:
            logger.error(f"Error cleaning up aggregations: {e}")
    
    def set_aggregation_timeout(self, seconds: int):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–∞–π–º–∞—É—Ç –∞–≥—Ä–µ–≥–∞—Ü–∏–∏"""
        if seconds > 0:
            self.aggregation_timeout = seconds
            logger.info(f"Aggregation timeout set to {seconds} seconds")

    async def _save_task_to_redis(self, task_data: dict) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–∞–¥–∞—á—É –≤ Redis –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑ redis_client
            task_id = await self.redis.save_task(task_data)
            logger.info(f"Task {task_id} saved to Redis successfully")
            return task_id
            
        except Exception as e:
            logger.error(f"Error saving task to Redis: {e}")
            raise

    async def _publish_task_event(self, task_id: str):
        """–ü—É–±–ª–∏–∫—É–µ—Ç —Å–æ–±—ã—Ç–∏–µ –æ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–µ"""
        try:
            await self.redis._ensure_connection()
            
            event_data = {
                "type": "new_task",
                "task_id": task_id
            }
            
            await self.redis.conn.publish("new_tasks", json.dumps(event_data))
            
        except Exception as e:
            logger.error(f"Error publishing task event: {e}")

    async def _increment_counter(self, counter_name: str):
        """–£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Å—á–µ—Ç—á–∏–∫"""
        try:
            await self.redis._ensure_connection()
            await self.redis.conn.incr(f"counter:{counter_name}")
            
        except Exception as e:
            logger.error(f"Error incrementing counter {counter_name}: {e}")
